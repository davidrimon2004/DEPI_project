{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed14c1d1",
   "metadata": {
    "papermill": {
     "duration": 0.002873,
     "end_time": "2025-11-28T08:45:53.695243",
     "exception": false,
     "start_time": "2025-11-28T08:45:53.692370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://www.kaggle.com/code/davidhalim2004/data-cleaning?scriptVersionId=280113860\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696120fe",
   "metadata": {
    "papermill": {
     "duration": 0.001968,
     "end_time": "2025-11-28T08:45:53.699588",
     "exception": false,
     "start_time": "2025-11-28T08:45:53.697620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522ba589",
   "metadata": {
    "_cell_guid": "c937e31b-4b1f-40d1-8db4-c9dfc4b99b42",
    "_kg_hide-output": true,
    "_uuid": "01f2ca21-b4c1-4e8d-b532-36804b5cc32e",
    "execution": {
     "iopub.execute_input": "2025-11-28T08:45:53.704566Z",
     "iopub.status.busy": "2025-11-28T08:45:53.704330Z",
     "iopub.status.idle": "2025-11-28T08:46:00.679386Z",
     "shell.execute_reply": "2025-11-28T08:46:00.678762Z"
    },
    "papermill": {
     "duration": 6.979195,
     "end_time": "2025-11-28T08:46:00.680832",
     "exception": false,
     "start_time": "2025-11-28T08:45:53.701637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->xgboost) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->xgboost) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->xgboost) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daff8aae",
   "metadata": {
    "papermill": {
     "duration": 0.002138,
     "end_time": "2025-11-28T08:46:00.685489",
     "exception": false,
     "start_time": "2025-11-28T08:46:00.683351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# data pre-proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cf318",
   "metadata": {
    "papermill": {
     "duration": 0.001935,
     "end_time": "2025-11-28T08:46:00.689720",
     "exception": false,
     "start_time": "2025-11-28T08:46:00.687785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4239492d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:46:00.695068Z",
     "iopub.status.busy": "2025-11-28T08:46:00.694711Z",
     "iopub.status.idle": "2025-11-28T08:47:20.172152Z",
     "shell.execute_reply": "2025-11-28T08:47:20.171404Z"
    },
    "papermill": {
     "duration": 79.481867,
     "end_time": "2025-11-28T08:47:20.173819",
     "exception": false,
     "start_time": "2025-11-28T08:46:00.691952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(\"/kaggle/input/depi-dataset/data.csv\", chunksize=5_000_000)\n",
    "final_data = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8116420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:47:20.179938Z",
     "iopub.status.busy": "2025-11-28T08:47:20.179283Z",
     "iopub.status.idle": "2025-11-28T08:47:20.183186Z",
     "shell.execute_reply": "2025-11-28T08:47:20.182473Z"
    },
    "papermill": {
     "duration": 0.007847,
     "end_time": "2025-11-28T08:47:20.184250",
     "exception": false,
     "start_time": "2025-11-28T08:47:20.176403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c416e336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:47:20.189402Z",
     "iopub.status.busy": "2025-11-28T08:47:20.189028Z",
     "iopub.status.idle": "2025-11-28T08:47:20.200203Z",
     "shell.execute_reply": "2025-11-28T08:47:20.199533Z"
    },
    "papermill": {
     "duration": 0.015006,
     "end_time": "2025-11-28T08:47:20.201382",
     "exception": false,
     "start_time": "2025-11-28T08:47:20.186376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator OrdinalEncoder from version 1.7.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.7.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 1.7.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator ColumnTransformer from version 1.7.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocessor=joblib.load(\"/kaggle/input/depi-dataset/pre-proccess.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5254b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T08:47:20.206824Z",
     "iopub.status.busy": "2025-11-28T08:47:20.206426Z",
     "iopub.status.idle": "2025-11-28T08:47:20.213805Z",
     "shell.execute_reply": "2025-11-28T08:47:20.213222Z"
    },
    "papermill": {
     "duration": 0.011185,
     "end_time": "2025-11-28T08:47:20.214888",
     "exception": false,
     "start_time": "2025-11-28T08:47:20.203703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/kaggle/working/pre-proccess.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols = []\n",
    "numeric_cols = [\"wm_yr_wk\", \"wday\", \"snap\",\"year\", \"month\", \"day\", \"sell_price\",\"lag_1\",'price_flag','lag_7','snap_weekend','wday_x_snap','is_weekend','event_impact','event_count','is_event']\n",
    "categorical_cols += [\"item_category\", \"item_subcategory\",'item_number']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "numeric_transformer = \"passthrough\"\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"cat\", categorical_transformer, categorical_cols),\n",
    "    (\"num\", numeric_transformer, numeric_cols)\n",
    "])\n",
    "joblib.dump(preprocessor,\"/kaggle/working/pre-proccess.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d04b10c",
   "metadata": {
    "papermill": {
     "duration": 0.002142,
     "end_time": "2025-11-28T08:47:20.219608",
     "exception": false,
     "start_time": "2025-11-28T08:47:20.217466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4948c72",
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2025-11-28T08:47:20.224957Z",
     "iopub.status.busy": "2025-11-28T08:47:20.224754Z",
     "iopub.status.idle": "2025-11-28T09:26:41.716883Z",
     "shell.execute_reply": "2025-11-28T09:26:41.716159Z"
    },
    "papermill": {
     "duration": 2361.499052,
     "end_time": "2025-11-28T09:26:41.720955",
     "exception": false,
     "start_time": "2025-11-28T08:47:20.221903",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training model for store: CA_1 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [08:50:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train: 0.3922, test: 0.4169\n",
      "R2  train: 0.9237, test: 0.8832\n",
      "Model for CA_1 trained and stored.\n",
      "\n",
      "==== Training model for store: CA_2 ====\n",
      "MAE train: 0.3162, test: 0.3354\n",
      "R2  train: 0.8884, test: 0.8452\n",
      "Model for CA_2 trained and stored.\n",
      "\n",
      "==== Training model for store: CA_3 ====\n",
      "MAE train: 0.5363, test: 0.5693\n",
      "R2  train: 0.9357, test: 0.9138\n",
      "Model for CA_3 trained and stored.\n",
      "\n",
      "==== Training model for store: CA_4 ====\n",
      "MAE train: 0.2345, test: 0.2482\n",
      "R2  train: 0.8845, test: 0.8437\n",
      "Model for CA_4 trained and stored.\n",
      "\n",
      "==== Training model for store: TX_1 ====\n",
      "MAE train: 0.2963, test: 0.3184\n",
      "R2  train: 0.9249, test: 0.8559\n",
      "Model for TX_1 trained and stored.\n",
      "\n",
      "==== Training model for store: TX_2 ====\n",
      "MAE train: 0.3609, test: 0.3849\n",
      "R2  train: 0.9407, test: 0.9101\n",
      "Model for TX_2 trained and stored.\n",
      "\n",
      "==== Training model for store: TX_3 ====\n",
      "MAE train: 0.3139, test: 0.3361\n",
      "R2  train: 0.9352, test: 0.9025\n",
      "Model for TX_3 trained and stored.\n",
      "\n",
      "==== Training model for store: WI_1 ====\n",
      "MAE train: 0.2821, test: 0.2979\n",
      "R2  train: 0.8948, test: 0.8687\n",
      "Model for WI_1 trained and stored.\n",
      "\n",
      "==== Training model for store: WI_2 ====\n",
      "MAE train: 0.3486, test: 0.3789\n",
      "R2  train: 0.9200, test: 0.8804\n",
      "Model for WI_2 trained and stored.\n",
      "\n",
      "==== Training model for store: WI_3 ====\n",
      "MAE train: 0.3243, test: 0.3510\n",
      "R2  train: 0.9367, test: 0.8987\n",
      "Model for WI_3 trained and stored.\n"
     ]
    }
   ],
   "source": [
    "store_models = {}\n",
    "\n",
    "stores = final_data[\"store_id\"].unique()\n",
    "\n",
    "for store in stores:\n",
    "    print(f\"\\n==== Training model for store: {store} ====\")\n",
    "\n",
    "    # 1️⃣ Filter the data of that store only\n",
    "    df_store = final_data[final_data[\"store_id\"] == store].copy()\n",
    "\n",
    "    # 2️⃣ Split into X, y\n",
    "    X = df_store.drop(columns=[\"sales\"])\n",
    "    y = df_store[\"sales\"]\n",
    "    X.drop(columns='store_id',inplace=True)\n",
    "    X=preprocessor.fit_transform(X)\n",
    "\n",
    "    # 3️⃣ Random Split for THIS store ONLY\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.15, random_state=42\n",
    "    )\n",
    "\n",
    "    # 4️⃣ Fit model\n",
    "    model = XGBRegressor(\n",
    "    objective='reg:tweedie',\n",
    "    tweedie_variance_power=1.35,\n",
    "    booster='gbtree',\n",
    "    device='cuda',\n",
    "    tree_method='hist',\n",
    "    random_state=42,\n",
    "    eval_metric='mae',\n",
    "    learning_rate=0.03,\n",
    "    max_depth=7,\n",
    "    min_child_weight=15,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=6,\n",
    "    reg_alpha=2,\n",
    "    n_estimators=5000,\n",
    "    early_stopping_rounds=40,\n",
    "    gamma= 1,\n",
    "    max_delta_step = 1\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose = False\n",
    "    )\n",
    "    \n",
    "    model.score(X_train,y_train) , model.score(X_test,y_test)\n",
    "    from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "    train_mae = mean_absolute_error(y_train, model.predict(X_train))\n",
    "    test_mae  = mean_absolute_error(y_test,  model.predict(X_test))\n",
    "    train_r2  = r2_score(y_train, model.predict(X_train))\n",
    "    test_r2   = r2_score(y_test,  model.predict(X_test))\n",
    "\n",
    "    print(f\"MAE train: {train_mae:.4f}, test: {test_mae:.4f}\")\n",
    "    print(f\"R2  train: {train_r2:.4f}, test: {test_r2:.4f}\")\n",
    "   \n",
    "    # 5️⃣ Save model\n",
    "    store_models[store] = model\n",
    "    print(f\"Model for {store} trained and stored.\")\n",
    "\n",
    "    del X, y, X_temp, X_test, X_val, X_train, y_train, y_temp, y_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255fc3ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T09:26:41.727984Z",
     "iopub.status.busy": "2025-11-28T09:26:41.727380Z",
     "iopub.status.idle": "2025-11-28T09:28:54.088785Z",
     "shell.execute_reply": "2025-11-28T09:28:54.087921Z"
    },
    "papermill": {
     "duration": 132.366294,
     "end_time": "2025-11-28T09:28:54.090166",
     "exception": false,
     "start_time": "2025-11-28T09:26:41.723872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   2   Actual:   2\n",
      "Pred:  11   Actual:   8\n",
      "Pred:   3   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   2\n",
      "Pred:   6   Actual:   6\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   3   Actual:   3\n",
      "Pred:   6   Actual:   6\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   3   Actual:   3\n",
      "Pred:  12   Actual:   6\n",
      "Pred:   2   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   3   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   4   Actual:   4\n",
      "Pred:   0   Actual:   0\n",
      "Pred:  17   Actual:  11\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   2   Actual:   2\n",
      "Pred:   6   Actual:   4\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   3   Actual:   4\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   5   Actual:   5\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   9   Actual:  14\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   2\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   3   Actual:   3\n",
      "Pred:   5   Actual:   5\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   2   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   3\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   5   Actual:   3\n",
      "Pred:   2   Actual:   1\n",
      "Pred:  11   Actual:  10\n",
      "Pred:   4   Actual:   5\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   6   Actual:  10\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   6   Actual:   8\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:  10   Actual:  24\n",
      "Pred:   4   Actual:   5\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   5   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   1\n",
      "Pred:   3   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   4   Actual:   5\n",
      "Pred:   8   Actual:   6\n",
      "Pred:   3   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   4\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   3   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   3   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   2   Actual:   1\n",
      "Pred:   2   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   2   Actual:   7\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   1   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   8   Actual:   5\n",
      "Pred:  12   Actual:  14\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   3\n",
      "Pred:   3   Actual:   3\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "\n",
      "=== Sample Predictions vs Actuals ===\n",
      "\n",
      "Pred:   4   Actual:   4\n",
      "Pred:   3   Actual:   3\n",
      "Pred:   5   Actual:   4\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   1\n",
      "Pred:   1   Actual:   1\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   1   Actual:   2\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   2   Actual:   1\n",
      "Pred:   3   Actual:   5\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n",
      "Pred:   0   Actual:   0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for store in stores:\n",
    "    # 1️⃣ Filter the data of that store only\n",
    "    df_store = final_data[final_data[\"store_id\"] == store].copy()\n",
    "\n",
    "    # 2️⃣ Split into X, y\n",
    "    X = df_store.drop(columns=[\"sales\"])\n",
    "    y = df_store[\"sales\"]\n",
    "    X=preprocessor.fit_transform(X)\n",
    "\n",
    "    # 3️⃣ Random Split for THIS store ONLY\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.15, random_state=42\n",
    "    )\n",
    "    y_pred = store_models[store].predict(X_test)\n",
    "    # Round predictions and ground truth\n",
    "    y_pred_rounded = np.round(y_pred).astype(int)\n",
    "    y_test_rounded = np.round(y_test).astype(int)\n",
    "\n",
    "    print(\"\\n=== Sample Predictions vs Actuals ===\\n\")\n",
    "    for i in range(min(25, len(y_test))):\n",
    "        print(f\"Pred: {y_pred_rounded[i]:>3}   Actual: {y_test_rounded.iloc[i]:>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79dac2d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-28T09:28:54.098036Z",
     "iopub.status.busy": "2025-11-28T09:28:54.097786Z",
     "iopub.status.idle": "2025-11-28T09:28:56.454410Z",
     "shell.execute_reply": "2025-11-28T09:28:56.453798Z"
    },
    "papermill": {
     "duration": 2.361895,
     "end_time": "2025-11-28T09:28:56.455741",
     "exception": false,
     "start_time": "2025-11-28T09:28:54.093846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in store_models.keys():\n",
    "    joblib.dump(store_models[i], f\"/kaggle/working/{i}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8485735,
     "sourceId": 13832954,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 490130,
     "modelInstanceId": 474247,
     "sourceId": 657262,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2587.307409,
   "end_time": "2025-11-28T09:28:57.378461",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-28T08:45:50.071052",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
